{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIvi+FDitXEFpwLdDMVpLa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak-Gautam/AudioDataPrerocess/blob/main/audio_pipeline_single_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get data in local file."
      ],
      "metadata": {
        "id": "tUFAjP4HY80N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t_NLceuX__L"
      },
      "outputs": [],
      "source": [
        "!pip install --force-reinstall torchaudio==2.3.0 -q\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "\n",
        "!pip install  git+https://github.com/pyannote/pyannote-audio.git@develop -q\n",
        "!pip install aiohttp aiofiles huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "from huggingface_hub import hf_hub_url, HfApi, hf_hub_download\n",
        "from tqdm.asyncio import tqdm_asyncio"
      ],
      "metadata": {
        "id": "DX5ZkRX9Y4bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def download_file(session, file, repo_id, repo_type, destination_dir, semaphore):\n",
        "    async with semaphore:\n",
        "        file_url = hf_hub_url(repo_id, file, repo_type=repo_type)\n",
        "        dest_path = os.path.join(destination_dir, file)\n",
        "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
        "\n",
        "        async with session.get(file_url) as response:\n",
        "            if response.status == 200:\n",
        "                async with aiofiles.open(dest_path, 'wb') as f:\n",
        "                    await f.write(await response.read())\n",
        "            else:\n",
        "                print(f\"Failed to download {file}: HTTP {response.status}\")\n",
        "\n",
        "async def download_dataset(repo_id, repo_type, folder_path, destination_dir, max_concurrent=10):\n",
        "    api = HfApi()\n",
        "    all_files = api.list_repo_files(repo_id, repo_type=repo_type)\n",
        "    folder_files = [f for f in all_files if f.startswith(folder_path)]\n",
        "\n",
        "    semaphore = asyncio.Semaphore(max_concurrent)\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [\n",
        "            download_file(session, file, repo_id, repo_type, destination_dir, semaphore)\n",
        "            for file in folder_files\n",
        "        ]\n",
        "        await tqdm_asyncio.gather(*tasks, desc=\"Downloading files\")\n",
        "\n",
        "# Configuration\n",
        "repo_id = \"Alignment-Lab-AI/podcast-1-test-preprocessed\"\n",
        "repo_type = \"dataset\"\n",
        "folder_path = \"0\"\n",
        "destination_dir = \"content/ddata\"\n",
        "\n",
        "# Run the async function\n",
        "async def main():\n",
        "    await download_dataset(repo_id, repo_type, folder_path, destination_dir)\n",
        "    print(f\"Folder '{folder_path}' from repository '{repo_id}' has been saved to '{destination_dir}'\")\n",
        "\n",
        "# This part is changed to work in Jupyter/Colab\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "loop = asyncio.get_event_loop()\n",
        "loop.run_until_complete(main())"
      ],
      "metadata": {
        "id": "tfpkx_0OY6RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub -q"
      ],
      "metadata": {
        "id": "NRNNqxvzMniz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a static FFmpeg build and add it to PATH.\n",
        "exist = !which ffmpeg\n",
        "if not exist:\n",
        "  !curl https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz -o ffmpeg.tar.xz \\\n",
        "     && tar -xf ffmpeg.tar.xz && rm ffmpeg.tar.xz\n",
        "  ffmdir = !find . -iname ffmpeg-*-static\n",
        "  path = %env PATH\n",
        "  path = path + ':' + ffmdir[0]\n",
        "  %env PATH $path\n",
        "print('')\n",
        "!which ffmpeg\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "we3G6EHHMpDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing starts from here!"
      ],
      "metadata": {
        "id": "bU1prri-ZCRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "HF_TOKEN = \"...\"\n",
        "INPUT_FOLDER = \"content/ddata/0\"\n",
        "OUTPUT_FOLDER = \"contet/autodiarization\"\n",
        "SPACER_DURATION = 2000  # milliseconds"
      ],
      "metadata": {
        "id": "TJITgxekMt-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "KDZoM_b7tzbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "import shutil\n",
        "import whisper\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipeline.to(device)"
      ],
      "metadata": {
        "id": "ge4dxfSwM4Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Whisper model\n",
        "distil_mod = hf_hub_download(repo_id=\"distil-whisper/distil-large-v2\", filename=\"original-model.fp32.bin\")\n",
        "whisper_model = whisper.load_model(distil_mod, device=device)"
      ],
      "metadata": {
        "id": "x0H9lOGBNAHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_directory(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def transcribe_audio(audio_path: str) -> str:\n",
        "    result = whisper_model.transcribe(audio=audio_path, language='en')\n",
        "    return result['text']\n",
        "\n",
        "def process_audio_file(input_file: str, output_dir: str) -> List[Tuple[str, str, str]]:\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "    print(f\"The audio is: {len(audio) / 1000.0} seconds\")\n",
        "    spacer = AudioSegment.silent(duration=SPACER_DURATION)\n",
        "    audio = spacer.append(audio, crossfade=0)\n",
        "\n",
        "    temp_wav = os.path.join(output_dir, \"temp.wav\")\n",
        "    audio.export(temp_wav, format='wav')\n",
        "\n",
        "    diarization = pipeline(temp_wav)\n",
        "\n",
        "    segments = []\n",
        "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        start = int(turn.start * 1000)\n",
        "        end = int(turn.end * 1000)\n",
        "        segment = audio[start:end]\n",
        "\n",
        "        speaker_dir = os.path.join(output_dir, f\"speaker_{speaker.lower()}\")\n",
        "        create_directory(speaker_dir)\n",
        "\n",
        "        segment_filename = f\"speaker_{speaker}_{len(segments):03d}.wav\"\n",
        "        segment_path = os.path.join(speaker_dir, segment_filename)\n",
        "        segment.export(segment_path, format='wav')\n",
        "\n",
        "        # Transcribe the segment\n",
        "        transcription = transcribe_audio(segment_path)\n",
        "\n",
        "        segments.append((segment_filename, f\"Speaker {speaker}\", transcription))\n",
        "\n",
        "    os.remove(temp_wav)\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "B4IjGAlUNDGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_metadata(speaker_dir: str, segments: List[Tuple[str, str, str]]):\n",
        "    create_directory(speaker_dir)\n",
        "    metadata_path = os.path.join(speaker_dir, \"metadata.csv\")\n",
        "    with open(metadata_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile, delimiter='|')\n",
        "        writer.writerow([\"filename\", \"speaker\", \"text\"])\n",
        "        for segment in segments:\n",
        "            writer.writerow([segment[0].split('.')[0], segment[1], segment[2]])"
      ],
      "metadata": {
        "id": "kIiQOXJcNICy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def main():\n",
        "    create_directory(OUTPUT_FOLDER)\n",
        "    for i, filename in enumerate(os.listdir(INPUT_FOLDER)):\n",
        "        if filename.endswith(('.mp3', '.wav', '.flac')):\n",
        "            input_file = os.path.join(INPUT_FOLDER, filename)\n",
        "            output_dir = os.path.join(OUTPUT_FOLDER, str(i).lower())\n",
        "            create_directory(output_dir)\n",
        "\n",
        "            print(f\"Processing file {i + 1}: {filename}\")\n",
        "            start = time.time()\n",
        "            segments = process_audio_file(input_file, output_dir)\n",
        "\n",
        "            speakers = set(segment[1] for segment in segments)\n",
        "            for speaker in speakers:\n",
        "                speaker_segments = [segment for segment in segments if segment[1] == speaker]\n",
        "                speaker_dir = os.path.join(output_dir, speaker.lower().replace(' ', '_'))\n",
        "                write_metadata(speaker_dir, speaker_segments)\n",
        "\n",
        "            stop = time.time()\n",
        "            print(f\"Finished processing file {i + 1}: {filename} in {stop-start} seconds.\")"
      ],
      "metadata": {
        "id": "hLgSFt9RNKPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "8qfHLw7gNM0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}